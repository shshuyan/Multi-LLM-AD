{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold Dataset Construction (10 Trials)\n",
        "\n",
        "## Objective\n",
        "After selecting the best upstream method from Study 2, extract all atomic criteria and manually annotate all fields using OpenAD schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Optional: install missing dependencies directly in the notebook\n",
        "! pip install --quiet seaborn matplotlib pandas numpy scipy scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "plt.rcParams[\"font.size\"] = 10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configured 12 trials for gold dataset seed extraction.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import textwrap\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "BASE_DIR = \"/Users/guoshuyan/Desktop/OpenAD\"\n",
        "RAW_DATA = os.path.join(BASE_DIR, \"Raw_data\")\n",
        "OUT_DIR = os.path.join(BASE_DIR, \"Study2_outputs\")\n",
        "SEED_OUTPUT_CSV = os.path.join(OUT_DIR, \"gold_seed_kimi_12_trials.csv\")\n",
        "ANNOTATION_TEMPLATE_CSV = os.path.join(OUT_DIR, \"gold_annotation_template_kimi.csv\")\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "GOLD_TRIAL_IDS = [\n",
        "    \"NCT01767311\",\n",
        "    \"NCT02008357\",\n",
        "    \"NCT02477800\",\n",
        "    \"NCT02484547\",\n",
        "    \"NCT03443973\",\n",
        "    \"NCT03444870\",\n",
        "    \"NCT03887455\",\n",
        "    \"NCT04437511\",\n",
        "    \"NCT04770220\",\n",
        "    \"NCT04777396\",\n",
        "    \"NCT05026866\",\n",
        "    \"NCT05108922\",\n",
        "]\n",
        "\n",
        "print(f\"Configured {len(GOLD_TRIAL_IDS)} trials for gold dataset seed extraction.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_json(path: str) -> Dict[str, Any]:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(path)\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "\n",
        "def extract_study_obj(data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    if not data:\n",
        "        return {}\n",
        "    if \"studies\" in data and isinstance(data[\"studies\"], list) and data[\"studies\"]:\n",
        "        return data[\"studies\"][0]\n",
        "    if \"protocolSection\" in data:\n",
        "        return data\n",
        "    if \"FullStudiesResponse\" in data:\n",
        "        try:\n",
        "            return data[\"FullStudiesResponse\"][\"FullStudies\"][0][\"Study\"]\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "\n",
        "def get_eligibility_text(study: Dict[str, Any]) -> str:\n",
        "    if not study:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return study[\"protocolSection\"][\"eligibilityModule\"][\"eligibilityCriteria\"] or \"\"\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        return study[\"ProtocolSection\"][\"EligibilityModule\"][\"EligibilityCriteria\"] or \"\"\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def split_inclusion_exclusion(raw_text: str) -> Dict[str, str]:\n",
        "    text = raw_text.replace(\"\\r\\n\", \"\\n\")\n",
        "    lower = text.lower()\n",
        "    inc_idx = lower.find(\"inclusion criteria\")\n",
        "    exc_idx = lower.find(\"exclusion criteria\")\n",
        "\n",
        "    inclusion = \"\"\n",
        "    exclusion = \"\"\n",
        "\n",
        "    if inc_idx != -1:\n",
        "        if exc_idx != -1:\n",
        "            inclusion = text[inc_idx:exc_idx].strip()\n",
        "            exclusion = text[exc_idx:].strip()\n",
        "        else:\n",
        "            inclusion = text[inc_idx:].strip()\n",
        "    elif exc_idx != -1:\n",
        "        exclusion = text[exc_idx:].strip()\n",
        "    else:\n",
        "        inclusion = text.strip()\n",
        "\n",
        "    return {\"inclusion\": inclusion, \"exclusion\": exclusion}\n",
        "\n",
        "\n",
        "def build_segmentation_prompt(trial_id: str, criterion_type: str, section_text: str) -> str:\n",
        "    return textwrap.dedent(\n",
        "        f\"\"\"\n",
        "        You are an expert clinical trial information extraction system.\n",
        "\n",
        "        Task: From the following eligibility section of trial {trial_id}, extract a list of\n",
        "        atomic eligibility criteria of type \"{criterion_type}\".\n",
        "\n",
        "        \"Atomic\" means:\n",
        "          - Each criterion is a single logical condition.\n",
        "          - Do NOT merge multiple logical conditions into one sentence.\n",
        "          - Do NOT split a single logically unified sentence into tiny fragments.\n",
        "\n",
        "        Output format:\n",
        "          Return ONLY a valid JSON array, with NO extra text.\n",
        "          Each element must be an object with fields:\n",
        "            - \"trial_id\" (string)\n",
        "            - \"criterion_type\" (string: \"inclusion\" or \"exclusion\")\n",
        "            - \"source_sentence\" (string: the extracted sentence as it appears, with minimal edits)\n",
        "\n",
        "        Important:\n",
        "          - Preserve negations (\"no history of stroke\") and constraints.\n",
        "          - Do not invent criteria; only use text actually present.\n",
        "          - If the section is empty, return [].\n",
        "\n",
        "        Here is the section text:\n",
        "\n",
        "        \\\"\\\"\\\"{section_text}\\\"\\\"\\\"\n",
        "        \"\"\"\n",
        "    ).strip()\n",
        "\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "def call_kimi(prompt: str) -> str | None:\n",
        "    api_key = os.getenv(\"KIMI_API_KEY\") or os.getenv(\"MOONSHOT_API_KEY\")\n",
        "    if not api_key:\n",
        "        raise RuntimeError(\"KIMI_API_KEY or MOONSHOT_API_KEY not set.\")\n",
        "\n",
        "    client = OpenAI(api_key=api_key, base_url=\"https://api.moonshot.ai/v1\")\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=\"kimi-k2-turbo-preview\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are Kimi, an AI assistant that extracts atomic eligibility criteria.\"\n",
        "                    ),\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        return completion.choices[0].message.content.strip()\n",
        "    except Exception as exc:\n",
        "        print(f\"⚠️ Kimi API error: {exc}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def extract_json_array(text: str):\n",
        "    text = text.strip()\n",
        "    if text.startswith(\"```\"):\n",
        "        text = text.strip(\"`\")\n",
        "    start = text.find(\"[\")\n",
        "    end = text.rfind(\"]\")\n",
        "    if start == -1 or end == -1 or end <= start:\n",
        "        raise ValueError(\"No JSON array found in model output.\")\n",
        "    json_str = text[start : end + 1]\n",
        "    return json.loads(json_str)\n",
        "\n",
        "\n",
        "def llm_output_to_df(model: str, trial_id: str, criterion_type: str, raw_output: str) -> pd.DataFrame:\n",
        "    arr = extract_json_array(raw_output)\n",
        "    if not isinstance(arr, list):\n",
        "        raise ValueError(\"Expected a JSON list from model.\")\n",
        "\n",
        "    rows = []\n",
        "    for obj in arr:\n",
        "        if not isinstance(obj, dict):\n",
        "            continue\n",
        "        t_id = obj.get(\"trial_id\", trial_id)\n",
        "        ctype = (obj.get(\"criterion_type\", criterion_type) or \"\").lower()\n",
        "        sent = (obj.get(\"source_sentence\", \"\") or \"\").strip()\n",
        "        if not sent:\n",
        "            continue\n",
        "        rows.append(\n",
        "            {\n",
        "                \"model\": model,\n",
        "                \"trial_id\": t_id,\n",
        "                \"criterion_type\": ctype,\n",
        "                \"source_sentence\": sent,\n",
        "            }\n",
        "        )\n",
        "    return pd.DataFrame(rows)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_kimi_seed_extraction(trial_ids: List[str]) -> pd.DataFrame:\n",
        "    all_outputs: List[pd.DataFrame] = []\n",
        "\n",
        "    for tid in trial_ids:\n",
        "        json_path = os.path.join(RAW_DATA, f\"{tid}.json\")\n",
        "        data = load_json(json_path)\n",
        "        study = extract_study_obj(data)\n",
        "        eligibility = get_eligibility_text(study)\n",
        "        sections = split_inclusion_exclusion(eligibility)\n",
        "\n",
        "        for ctype in [\"inclusion\", \"exclusion\"]:\n",
        "            section_text = sections.get(ctype, \"\").strip()\n",
        "            if not section_text:\n",
        "                continue\n",
        "\n",
        "            prompt = build_segmentation_prompt(tid, ctype, section_text)\n",
        "            raw_output = call_kimi(prompt)\n",
        "            if not raw_output:\n",
        "                print(f\"⚠️ {tid} / {ctype}: no output returned by Kimi\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                df_out = llm_output_to_df(\"kimi\", tid, ctype, raw_output)\n",
        "                all_outputs.append(df_out)\n",
        "                print(f\"✓ {tid} / {ctype}: {len(df_out)} criteria\")\n",
        "            except Exception as exc:\n",
        "                print(f\"⚠️ {tid} / {ctype} parsing failed: {exc}\")\n",
        "\n",
        "    if not all_outputs:\n",
        "        return pd.DataFrame(columns=[\"model\", \"trial_id\", \"criterion_type\", \"source_sentence\"])\n",
        "\n",
        "    combined = pd.concat(all_outputs, ignore_index=True)\n",
        "    combined = combined.drop_duplicates(subset=[\"trial_id\", \"criterion_type\", \"source_sentence\"]).reset_index(drop=True)\n",
        "    return combined\n",
        "\n",
        "\n",
        "def create_annotation_template(seed_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    template = seed_df.copy()\n",
        "    template.insert(0, \"criterion_id\", [f\"KIMI-{i+1:04d}\" for i in range(len(template))])\n",
        "\n",
        "    annotation_fields = [\n",
        "        \"ad_domain\",\n",
        "        \"clinical_concept\",\n",
        "        \"operator\",\n",
        "        \"value_lower\",\n",
        "        \"value_upper\",\n",
        "        \"units\",\n",
        "        \"diagnostic_framework\",\n",
        "        \"severity_stage\",\n",
        "        \"temporal_scope\",\n",
        "        \"evidence_type\",\n",
        "        \"certainty\",\n",
        "    ]\n",
        "\n",
        "    for field in annotation_fields:\n",
        "        if field not in template.columns:\n",
        "            template[field] = \"\"\n",
        "\n",
        "    column_order = [\n",
        "        \"criterion_id\",\n",
        "        \"trial_id\",\n",
        "        \"criterion_type\",\n",
        "        \"source_sentence\",\n",
        "    ] + annotation_fields\n",
        "\n",
        "    template = template[column_order]\n",
        "    return template\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " KIMI_API_KEY     = ✔\n"
          ]
        }
      ],
      "source": [
        "KIMI_KEY     = \"sk-WwqYxLkX8kZPy2HTl3Msio8xGUv43YfOFSrbYyC7cbusSz3y\"\n",
        "os.environ[\"KIMI_API_KEY\"]    = KIMI_KEY\n",
        "print(\" KIMI_API_KEY     =\", \"✔\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ NCT01767311 / inclusion: 30 criteria\n",
            "✓ NCT01767311 / exclusion: 12 criteria\n",
            "✓ NCT02008357 / inclusion: 5 criteria\n",
            "✓ NCT02008357 / exclusion: 9 criteria\n",
            "✓ NCT02477800 / inclusion: 7 criteria\n",
            "✓ NCT02477800 / exclusion: 11 criteria\n",
            "✓ NCT02484547 / inclusion: 7 criteria\n",
            "✓ NCT02484547 / exclusion: 11 criteria\n",
            "✓ NCT03443973 / inclusion: 9 criteria\n",
            "✓ NCT03443973 / exclusion: 23 criteria\n",
            "✓ NCT03444870 / inclusion: 9 criteria\n",
            "✓ NCT03444870 / exclusion: 23 criteria\n",
            "✓ NCT03887455 / inclusion: 28 criteria\n",
            "✓ NCT03887455 / exclusion: 26 criteria\n",
            "✓ NCT04437511 / inclusion: 5 criteria\n",
            "✓ NCT04437511 / exclusion: 2 criteria\n",
            "✓ NCT04770220 / inclusion: 6 criteria\n",
            "✓ NCT04770220 / exclusion: 7 criteria\n",
            "✓ NCT04777396 / inclusion: 7 criteria\n",
            "✓ NCT04777396 / exclusion: 5 criteria\n",
            "✓ NCT05026866 / inclusion: 6 criteria\n",
            "✓ NCT05026866 / exclusion: 11 criteria\n",
            "✓ NCT05108922 / inclusion: 8 criteria\n",
            "✓ NCT05108922 / exclusion: 7 criteria\n",
            "Saved seed extraction to /Users/guoshuyan/Desktop/OpenAD/Study2_outputs/gold_seed_kimi_12_trials.csv\n",
            "Saved annotation template to /Users/guoshuyan/Desktop/OpenAD/Study2_outputs/gold_annotation_template_kimi.csv\n"
          ]
        }
      ],
      "source": [
        "# Uncomment the lines below to run the extraction and template generation inside the notebook.\n",
        "kimi_seed_df = run_kimi_seed_extraction(GOLD_TRIAL_IDS)\n",
        "kimi_seed_df.to_csv(SEED_OUTPUT_CSV, index=False)\n",
        "annotation_template_df = create_annotation_template(kimi_seed_df.drop(columns=[\"model\"], errors=\"ignore\"))\n",
        "annotation_template_df.to_csv(ANNOTATION_TEMPLATE_CSV, index=False)\n",
        "print(f\"Saved seed extraction to {SEED_OUTPUT_CSV}\")\n",
        "print(f\"Saved annotation template to {ANNOTATION_TEMPLATE_CSV}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "te-ml-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ablation Study 2: Gemini Extraction + Kimi-K2 Stack Depth\n",
        "\n",
        "## Objective\n",
        "Fix the extractor to Gemini-2.5 (best from Ablation 1), save the first-pass CSV, then study how 2-, 3-, and 5-round stacks of the Kimi-K2 verifier change quality/latency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "import io\n",
        "import time\n",
        "from typing import List, Dict, Any, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "BASE_DIR = \"/Users/guoshuyan/Desktop/OpenAD\"\n",
        "GOLD_DATA_CSV = os.path.join(BASE_DIR, \"gold_annotation.csv\")\n",
        "\n",
        "EXTRACTOR_MODEL = 'Gemini-2.5'\n",
        "VERIFIER_MODEL = 'Kimi-K2-Thinking'\n",
        "VERIFIER_ROUNDS = [2, 3, 5]\n",
        "FIRST_PASS_OUTPUT = os.path.join(BASE_DIR, \"ablation2_gemini_round1.csv\")\n",
        "SCHEMA_FIELDS = ['trial_id', 'source_sentence', 'criterion_type', 'ad_domain', 'clinical_concept', \n",
        "                 'operator', 'value_lower', 'value_upper', 'units', 'diagnostic_framework', \n",
        "                 'severity_stage', 'temporal_scope', 'evidence_type', 'certainty']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Extractor and Verifier Functions from Ablation Study 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import extractor and verifier prompt functions\n",
        "# (In practice, import from ablation1 module or define here)\n",
        "\n",
        "def create_extractor_prompt(trial_id: str, criterion_type: str, source_sentence: str) -> str:\n",
        "    \"\"\"Create CSV-based extractor prompt.\"\"\"\n",
        "    prompt = f\"\"\"You are an expert in Alzheimer's disease (AD) clinical trial eligibility extraction.\n",
        "\n",
        "Your task is to extract ONE structured criterion from a single sentence and output it as CSV.\n",
        "\n",
        "You MUST output a CSV table with:\n",
        "- EXACTLY one header row\n",
        "- EXACTLY one data row\n",
        "\n",
        "The columns MUST be in this exact order:\n",
        "\n",
        "trial_id,source_sentence,criterion_type,ad_domain,clinical_concept,operator,value_lower,value_upper,units,diagnostic_framework,severity_stage,temporal_scope,evidence_type,certainty\n",
        "\n",
        "TRIAL_ID: {trial_id}\n",
        "CRITERION_TYPE: {criterion_type}\n",
        "SENTENCE: {source_sentence}\n",
        "\n",
        "Output ONLY the CSV header row and the single data row.\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def create_verifier_prompt(source_sentence: str, extracted_csv_row: str) -> str:\n",
        "    \"\"\"Create CSV-based verifier prompt.\"\"\"\n",
        "    prompt = f\"\"\"You are verifying the correctness of a structured extraction of an AD clinical trial eligibility criterion.\n",
        "\n",
        "ORIGINAL SENTENCE: {source_sentence}\n",
        "EXTRACTED CSV ROW: {extracted_csv_row}\n",
        "\n",
        "For EACH field, output: field,present_in_text,is_correct,correct_value\n",
        "\n",
        "Output ONLY the CSV table.\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def call_llm_extractor(\n",
        "    model_name: str,\n",
        "    trial_id: str,\n",
        "    criterion_type: str,\n",
        "    source_sentence: str,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Call LLM for extraction (mocked for offline evaluation).\"\"\"\n",
        "    record = {field: '' for field in SCHEMA_FIELDS}\n",
        "    record['trial_id'] = trial_id\n",
        "    record['criterion_type'] = criterion_type\n",
        "    record['source_sentence'] = source_sentence\n",
        "    record['ad_domain'] = 'unspecified'\n",
        "    record['clinical_concept'] = 'unknown'\n",
        "    record['operator'] = '='\n",
        "    record['value_lower'] = ''\n",
        "    record['value_upper'] = ''\n",
        "    record['units'] = ''\n",
        "    record['diagnostic_framework'] = ''\n",
        "    record['severity_stage'] = ''\n",
        "    record['temporal_scope'] = ''\n",
        "    record['evidence_type'] = 'text'\n",
        "    record['certainty'] = 'unknown'\n",
        "    record['model'] = model_name\n",
        "    return record\n",
        "\n",
        "def call_llm_verifier(model_name: str, prompt: str) -> pd.DataFrame:\n",
        "    \"\"\"Call LLM for verification.\"\"\"\n",
        "    # Mock implementation\n",
        "    mock_csv = \"\"\"field,present_in_text,is_correct,correct_value\n",
        "trial_id,true,true,NCT123\n",
        "source_sentence,true,true,\"MMSE score ≥ 20\"\n",
        "criterion_type,true,true,inclusion\n",
        "ad_domain,true,true,cognitive\n",
        "clinical_concept,true,true,MMSE\n",
        "operator,true,true,≥\n",
        "value_lower,true,true,20\"\"\"\n",
        "    return pd.read_csv(io.StringIO(mock_csv))\n",
        "\n",
        "def parse_csv_response(csv_text: str) -> Dict[str, Any]:\n",
        "    \"\"\"Parse CSV extraction response.\"\"\"\n",
        "    reader = csv.DictReader(io.StringIO(csv_text))\n",
        "    return next(reader, {})\n",
        "\n",
        "def apply_verifier_corrections(extracted: Dict[str, Any], verification_df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"Apply verifier corrections.\"\"\"\n",
        "    corrected = extracted.copy()\n",
        "    for _, row in verification_df.iterrows():\n",
        "        field = row['field']\n",
        "        if row['is_correct'] == False and pd.notna(row.get('correct_value', '')):\n",
        "            corrected[field] = row['correct_value']\n",
        "    return corrected\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacked Extraction Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_stacked_extraction(\n",
        "    trial_id: str,\n",
        "    criterion_type: str,\n",
        "    source_sentence: str,\n",
        "    verifier_rounds: int,\n",
        "    initial_result: Optional[Dict[str, Any]] = None,\n",
        ") -> tuple:\n",
        "    \"\"\"Run stacked extraction with Gemini first-pass + stacked Kimi verifiers.\"\"\"\n",
        "    start_time = time.time()\n",
        "    tokens_used = 0\n",
        "\n",
        "    # Initial extraction (Gemini) if not provided\n",
        "    if initial_result is None:\n",
        "        extractor_prompt = create_extractor_prompt(trial_id, criterion_type, source_sentence)\n",
        "        extracted = call_llm_extractor(EXTRACTOR_MODEL, trial_id, criterion_type, source_sentence)\n",
        "        tokens_used += len(extractor_prompt.split()) + 100  # Estimate\n",
        "    else:\n",
        "        extracted = initial_result.copy()\n",
        "\n",
        "    current_result = extracted.copy()\n",
        "    all_verifications = []\n",
        "\n",
        "    # Stack Kimi verifier rounds\n",
        "    for round_idx in range(verifier_rounds):\n",
        "        csv_row = ','.join([str(current_result.get(field, '')) for field in SCHEMA_FIELDS])\n",
        "        verifier_prompt = create_verifier_prompt(source_sentence, csv_row)\n",
        "        verification_df = call_llm_verifier(VERIFIER_MODEL, verifier_prompt)\n",
        "        verification_df['verifier_round'] = round_idx + 1\n",
        "        tokens_used += len(verifier_prompt.split()) + 50  # Estimate\n",
        "\n",
        "        all_verifications.append(verification_df)\n",
        "        current_result = apply_verifier_corrections(current_result, verification_df)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    return current_result, all_verifications, elapsed_time, tokens_used\n",
        "\n",
        "def calculate_field_accuracy(predicted: Dict[str, Any], gold: pd.Series) -> float:\n",
        "    \"\"\"Calculate field-level accuracy.\"\"\"\n",
        "    gold_non_empty = gold[gold.notna()]\n",
        "    if len(gold_non_empty) == 0:\n",
        "        return 1.0\n",
        "    \n",
        "    correct = 0\n",
        "    for field in gold_non_empty.index:\n",
        "        if field in predicted:\n",
        "            pred_val = str(predicted[field]).strip().lower()\n",
        "            gold_val = str(gold[field]).strip().lower()\n",
        "            if pred_val == gold_val:\n",
        "                correct += 1\n",
        "    \n",
        "    return correct / len(gold_non_empty) if len(gold_non_empty) > 0 else 0.0\n",
        "\n",
        "def calculate_over_correction_rate(initial: Dict[str, Any], final: Dict[str, Any], gold: pd.Series) -> float:\n",
        "    \"\"\"Calculate over-correction rate (correct fields changed to incorrect).\"\"\"\n",
        "    initial_correct = 0\n",
        "    final_correct = 0\n",
        "    total_fields = 0\n",
        "    \n",
        "    for field in SCHEMA_FIELDS:\n",
        "        if pd.notna(gold.get(field, np.nan)):\n",
        "            total_fields += 1\n",
        "            gold_val = str(gold[field]).strip().lower()\n",
        "            \n",
        "            initial_val = str(initial.get(field, '')).strip().lower()\n",
        "            final_val = str(final.get(field, '')).strip().lower()\n",
        "            \n",
        "            if initial_val == gold_val:\n",
        "                initial_correct += 1\n",
        "            if final_val == gold_val:\n",
        "                final_correct += 1\n",
        "    \n",
        "    if total_fields == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    over_corrected = initial_correct - final_correct\n",
        "    return max(0, over_corrected) / total_fields if total_fields > 0 else 0.0\n",
        "\n",
        "def calculate_disagreement_reduction(verifications: List[pd.DataFrame]) -> float:\n",
        "    \"\"\"Calculate disagreement reduction across verifier layers.\"\"\"\n",
        "    if len(verifications) < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    # Count fields where verifiers disagree\n",
        "    disagreements = 0\n",
        "    total_fields = len(SCHEMA_FIELDS)\n",
        "    \n",
        "    for field in SCHEMA_FIELDS:\n",
        "        field_values = []\n",
        "        for vdf in verifications:\n",
        "            field_row = vdf[vdf['field'] == field]\n",
        "            if len(field_row) > 0:\n",
        "                field_values.append(field_row.iloc[0].get('is_correct', True))\n",
        "        \n",
        "        # Check if all agree\n",
        "        if len(set(field_values)) > 1:\n",
        "            disagreements += 1\n",
        "    \n",
        "    return 1.0 - (disagreements / total_fields) if total_fields > 0 else 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gemini First-Pass Extraction\n",
        "Generate (and cache) the single-pass Gemini CSV that seeds all verifier stacks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_gemini_first_pass(dataset: pd.DataFrame, output_path: str = FIRST_PASS_OUTPUT) -> pd.DataFrame:\n",
        "    \"\"\"Run Gemini once per row, persist the CSV, and return the DataFrame.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        cached = pd.read_csv(output_path)\n",
        "        if len(cached) == len(dataset):\n",
        "            print(f\"Found cached Gemini outputs at {output_path}; reusing them.\")\n",
        "            return cached\n",
        "        print(\"Cached Gemini outputs found but row count mismatches dataset; re-generating.\")\n",
        "\n",
        "    outputs: List[Dict[str, Any]] = []\n",
        "\n",
        "    for _, row in dataset.iterrows():\n",
        "        prompt = create_extractor_prompt(row['trial_id'], row['criterion_type'], row['source_sentence'])\n",
        "        extraction = call_llm_extractor(\n",
        "            EXTRACTOR_MODEL,\n",
        "            row['trial_id'],\n",
        "            row['criterion_type'],\n",
        "            row['source_sentence'],\n",
        "        )\n",
        "\n",
        "        normalized = {field: extraction.get(field, '') for field in SCHEMA_FIELDS}\n",
        "        normalized['trial_id'] = row['trial_id']\n",
        "        normalized['criterion_type'] = row['criterion_type']\n",
        "        normalized['source_sentence'] = row['source_sentence']\n",
        "        outputs.append(normalized)\n",
        "\n",
        "    first_pass_df = pd.DataFrame(outputs)\n",
        "    first_pass_df.to_csv(output_path, index=False)\n",
        "    print(f\"Saved Gemini first-pass outputs to {output_path} ({len(first_pass_df)} rows)\")\n",
        "    return first_pass_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_stack_depth(\n",
        "    trial_id: str,\n",
        "    criterion_type: str,\n",
        "    source_sentence: str,\n",
        "    gold_row: pd.Series,\n",
        "    verifier_rounds: int,\n",
        "    initial_extraction: Dict[str, Any],\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate Gemini first-pass + stacked Kimi verifier rounds.\"\"\"\n",
        "    single_pass_result = initial_extraction.copy()\n",
        "    single_pass_accuracy = calculate_field_accuracy(single_pass_result, gold_row)\n",
        "\n",
        "    stacked_result, verifications, elapsed_time, tokens_used = run_stacked_extraction(\n",
        "        trial_id,\n",
        "        criterion_type,\n",
        "        source_sentence,\n",
        "        verifier_rounds,\n",
        "        initial_result=single_pass_result,\n",
        "    )\n",
        "    stacked_accuracy = calculate_field_accuracy(stacked_result, gold_row)\n",
        "\n",
        "    stack_improvement = stacked_accuracy - single_pass_accuracy\n",
        "    over_correction = calculate_over_correction_rate(single_pass_result, stacked_result, gold_row)\n",
        "    disagreement_reduction = calculate_disagreement_reduction(verifications)\n",
        "\n",
        "    return {\n",
        "        'verifier_rounds': verifier_rounds,\n",
        "        'trial_id': trial_id,\n",
        "        'single_pass_accuracy': single_pass_accuracy,\n",
        "        'stacked_accuracy': stacked_accuracy,\n",
        "        'stack_improvement': stack_improvement,\n",
        "        'over_correction_rate': over_correction,\n",
        "        'disagreement_reduction': disagreement_reduction,\n",
        "        'elapsed_time': elapsed_time,\n",
        "        'tokens_used': tokens_used,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_ablation2_evaluation(sample_size: Optional[int] = None) -> pd.DataFrame:\n",
        "    \"\"\"Run Ablation 2: Gemini extraction cached, then stacked Kimi verifiers.\"\"\"\n",
        "    if not os.path.exists(GOLD_DATA_CSV):\n",
        "        raise FileNotFoundError(f\"Gold dataset not found at {GOLD_DATA_CSV}\")\n",
        "\n",
        "    gold_df = pd.read_csv(GOLD_DATA_CSV)\n",
        "    if sample_size is not None:\n",
        "        gold_df = gold_df.head(sample_size)\n",
        "\n",
        "    if gold_df.empty:\n",
        "        raise ValueError(f\"Gold dataset is empty – check {GOLD_DATA_CSV}\")\n",
        "\n",
        "    first_pass_df = run_gemini_first_pass(gold_df, output_path=FIRST_PASS_OUTPUT)\n",
        "    first_pass_lookup = {\n",
        "        (row['trial_id'], row['criterion_type'], row['source_sentence']): row.to_dict()\n",
        "        for _, row in first_pass_df.iterrows()\n",
        "    }\n",
        "\n",
        "    all_results = []\n",
        "    for verifier_rounds in VERIFIER_ROUNDS:\n",
        "        print(f\"Evaluating {verifier_rounds} Kimi verifier rounds…\")\n",
        "        for _, gold_row in gold_df.iterrows():\n",
        "            key = (gold_row['trial_id'], gold_row['criterion_type'], gold_row['source_sentence'])\n",
        "            initial_row = first_pass_lookup.get(key)\n",
        "            if initial_row is None:\n",
        "                continue\n",
        "\n",
        "            initial_extraction = {field: initial_row.get(field, '') for field in SCHEMA_FIELDS}\n",
        "            result = evaluate_stack_depth(\n",
        "                gold_row['trial_id'],\n",
        "                gold_row['criterion_type'],\n",
        "                gold_row['source_sentence'],\n",
        "                gold_row,\n",
        "                verifier_rounds,\n",
        "                initial_extraction,\n",
        "            )\n",
        "            all_results.append(result)\n",
        "\n",
        "    return pd.DataFrame(all_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found cached Gemini outputs at /Users/guoshuyan/Desktop/OpenAD/ablation2_gemini_round1.csv; reusing them.\n",
            "Evaluating 2 Kimi verifier rounds…\n",
            "Evaluating 3 Kimi verifier rounds…\n",
            "Evaluating 5 Kimi verifier rounds…\n",
            "Evaluation complete. Results saved to ablation2_results.csv\n",
            "Gemini first-pass CSV saved to /Users/guoshuyan/Desktop/OpenAD/ablation2_gemini_round1.csv\n",
            "\n",
            "Results summary (by Kimi verifier rounds):\n",
            "                 stacked_accuracy  stack_improvement  over_correction_rate  \\\n",
            "verifier_rounds                                                              \n",
            "2                           0.284                0.0                   0.0   \n",
            "3                           0.284                0.0                   0.0   \n",
            "5                           0.284                0.0                   0.0   \n",
            "\n",
            "                 disagreement_reduction  elapsed_time  tokens_used  \n",
            "verifier_rounds                                                     \n",
            "2                                   1.0         0.001      240.206  \n",
            "3                                   1.0         0.001      360.309  \n",
            "5                                   1.0         0.002      600.515  \n"
          ]
        }
      ],
      "source": [
        "results_df = run_ablation2_evaluation()\n",
        "results_df.to_csv('ablation2_results.csv', index=False)\n",
        "print(\"Evaluation complete. Results saved to ablation2_results.csv\")\n",
        "print(\"Gemini first-pass CSV saved to\", FIRST_PASS_OUTPUT)\n",
        "print(\"\\nResults summary (by Kimi verifier rounds):\")\n",
        "print(results_df.groupby('verifier_rounds').agg({\n",
        "    'stacked_accuracy': 'mean',\n",
        "    'stack_improvement': 'mean',\n",
        "    'over_correction_rate': 'mean',\n",
        "    'disagreement_reduction': 'mean',\n",
        "    'elapsed_time': 'mean',\n",
        "    'tokens_used': 'mean'\n",
        "}).round(3))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "te-ml-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
